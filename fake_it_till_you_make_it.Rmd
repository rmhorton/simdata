---
title: "Fake It Till You Make It"
subtitle: "Simulating Data to Learn About ML"
author: "Bob Horton"
date: "10/13/2020"
output: html_document
params:
   random_seed: 42
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```

```{r libraries, message=FALSE}

library(dplyr)
library(randomForest)
library(glmnet)
library(ROCR)
library(ggplot2)

set.seed(params$random_seed)
options(width=120)

```

## Targeting the Edges of the ROC Curve

Different learning algorithms can learn different signals from a dataset. Here I put different signals in a simulataed dataset so that a linear model (`glmnet`) will have better performance on one edge of the ROC curve, and a tree-based (`randomForest`) model will have better performance on the other edge.


```{r simdata}

simulate_data <- function(num_rows=1000, num_categorical=5, cardinality=3, num_numeric=10, 
                          noise=0, y_threshold_quantile=0.95, y_fun=NULL, ...){
  
  simdata <- data.frame(id=1:num_rows)
  
  for (i in 1:num_categorical){
    col_name <- sprintf("cat%02d", i)
    simdata[[col_name]] <- sample(LETTERS[1:cardinality], num_rows, replace=TRUE)
  }
  
  for (i in 1:num_numeric){
    col_name <- sprintf("num%02d", i)
    simdata[[col_name]] <- rnorm(num_rows)
  }
  
  if (!is.null(y_fun)){
    simdata[['y']] <- y_fun(simdata, ...) + rnorm(num_rows, sd=noise)
    simdata[['y_cat']] <- simdata[['y']] > quantile(simdata[['y']], probs=y_threshold_quantile)
  }
  
  simdata
}

```


```{r outcome_function}

y_edges <- function(df, numeric_term_weight, interaction_weight){
  with(df, {
    numeric_term_weight*(0.6*num01 + 0.7*num02 + 0.8*num03 + 0.9*num04 + 1.0*num05 + 
                         1.1*num06 + 1.2*num07 + 1.3*num08 + 1.4*num09 + 1.5*num10) + 
      interaction_weight * ifelse(cat01 == cat02, 1, -1)
  })
}


```

```{r compare_models}

randomForest_vs_glmnet <- function(df_train, df_test, label, predictors){

  form <- formula(paste(label, paste(predictors, collapse=' + '), sep=' ~ '))

  X_train <- model.matrix(form, df_train)
  y_train <- as.numeric(df_train[[label]])

  X_test <- model.matrix(form, df_test)
  y_test <- as.numeric(df_test[[label]])
  
  fit_rf <- randomForest(X_train, factor(y_train), ntree=101)
  pred_rf <- predict(fit_rf, X_test, type='prob')[,2]
  rf_pred_obj <- prediction(pred_rf, y_test)
  
  fit_glmnet <- cv.glmnet(X_train, y_train,  nfolds=5)
  pred_glmnet <- predict(fit_glmnet, newx=X_test)[,1]
  glmnet_pred_obj <- prediction(pred_glmnet, y_test)
  
  plot(performance(rf_pred_obj, 'tpr', 'fpr'), col='blue', lwd=1.5)
  plot(performance(glmnet_pred_obj, 'tpr', 'fpr'), col='red', lwd=1.5, add=TRUE)
  
  rf_auc <- performance(rf_pred_obj,'auc')@y.values[[1]]
  glmnet_auc <- performance(glmnet_pred_obj,'auc')@y.values[[1]]
  
  legend_txt <- c(sprintf('%0.3f randomForest', rf_auc),  sprintf('%0.3f glmnet', glmnet_auc))
  legend('bottomright', legend=legend_txt, text.col=c('blue', 'red'), title='AUC', title.col='black', bty='n')
  
}

```

Here the `randomForest` model is better on the left edge, and the `glmnet` model is better on the top edge.

```{r rf_better_sensitivity}

sim_data_params <- list(num_rows=5000, num_categorical=5, cardinality=4, num_numeric=15, 
                        noise=2,
                        y_threshold_quantile=0.9, 
                        y_fun=y_edges, numeric_term_weight=4, interaction_weight=15)


df_train <- do.call(simulate_data, sim_data_params)
df_test <- do.call(simulate_data, sim_data_params)

label='y_cat'
predictors=setdiff(names(df_train), c('id', 'y', 'y_cat'))

randomForest_vs_glmnet( df_train, df_test, label, predictors)
```

Here the `glmnet` model is better on the left edge, and `randomForest` is better on the top edge.

```{r glmnet_better_sensitivity}

sim_data_params$y_threshold_quantile=0.5

df_train <- do.call(simulate_data, sim_data_params)
df_test <- do.call(simulate_data, sim_data_params)
  
randomForest_vs_glmnet( df_train, df_test, label, predictors)

```

Here the `glmnet` model is better on both edges, though the `randomForest` model has higher AUC.

```{r glmnet_better_both_ends}

sim_data_params$interaction_weight=12
sim_data_params$y_threshold_quantile=0.75

df_train <- do.call(simulate_data, sim_data_params)
df_test <- do.call(simulate_data, sim_data_params)
randomForest_vs_glmnet( df_train, df_test, label, predictors)


```

## Recursive Feature Elimination 

The `caret` package offers several options for estimating feature importance and selecting subsets of features to use as predictors. Here we'll try using Recursive Feature Selection with randomForest models.

Because [caret doesn't work if your binary response is of type _logical_](https://stackoverflow.com/questions/27303051/r-estimating-model-variance), we need to cas the outcome variable as a factor.

```{r rf_feature_importance}
library(randomForest)

df_train <- do.call(simulate_data, sim_data_params) %>% 
  mutate(y_cat=factor(y_cat, levels=c(TRUE, FALSE), labels=c("yes", "no")))

exclude <- c('id', 'y', 'y_cat')
feature_cols <- setdiff(names(df_train), exclude)

label <- 'y_cat'
form <- formula(paste(label, paste(feature_cols, collapse = ' + '), sep=' ~ '))

fit_rf <- randomForest(form, df_train, importance=TRUE)

importance(fit_rf)

```


Here I use only 3 cross-validation folds; normally you would use more (the default is 10), but that takes longer.

```{r caret_RFE}

library(caret)

control <- rfeControl(functions=rfFuncs, method="cv", number=3)
                 
x=df_train[,predictors]
y=df_train[[label]]
results <- rfe(x,y, sizes=c(1:15), rfeControl=control)

print(results)
plot(results)
predictors(results)

```


### Repeated numerical columns

Repeat the least important numerical column:

```{r repeated_num01}

repeat_column <- function(df, col, num_reps, suffix='rep'){
  for (i in 1:num_reps){
    new_col_name <- sprintf("%s_%s_%02d", col, suffix, i)
    df[[new_col_name]] <- df[[col]]
  }
  df
}

df_train_rep_num01 <- repeat_column(df_train, 'num01', 10)

feature_cols_rep_num01 <- setdiff(names(df_train_rep_num01), exclude)
form <- formula(paste(label, paste(feature_cols_rep_num01, collapse = ' + '), sep=' ~ '))


fit_rf_rep_num01 <- randomForest(form, df_train_rep_num01, importance=TRUE)

importance(fit_rf_rep_num01)

```

```{r caret_RFE_rep_num01}

x=df_train_rep_num01[, feature_cols_rep_num01]
y=df_train_rep_num01[[label]]
results <- rfe(x,y, sizes=c(1:15), rfeControl=control)

print(results)
plot(results)
predictors(results)

```


Repeat the most important numerical column:

```{r repeated_num10}

df_train_rep_num10 <- repeat_column(df_train, 'num10', 10)

feature_cols_rep_num10 <- setdiff(names(df_train_rep_num10), exclude)
form <- formula(paste(label, paste(feature_cols_rep_num10, collapse = ' + '), sep=' ~ '))


fit_rf_rep_num10 <- randomForest(form, df_train_rep_num10, importance=TRUE)

importance(fit_rf_rep_num10)

```

```{r caret_RFE_rep_num10}

x=df_train_rep_num10[, feature_cols_rep_num10]
y=df_train_rep_num10[[label]]
results <- rfe(x,y, sizes=c(1:15), rfeControl=control)

print(results)
plot(results)
predictors(results)

```

### Repeated categorical column

Repeat one of the interaction columns.

```{r repeated_categorical_columns}

df_train_rep_cat01 <- repeat_column(df_train, 'cat01', 10)

feature_cols_rep_cat01 <- setdiff(names(df_train_rep_cat01), exclude)
form <- formula(paste(label, paste(feature_cols_rep_cat01, collapse = ' + '), sep=' ~ '))


fit_rf_rep_cat01 <- randomForest(form, df_train_rep_cat01, importance=TRUE)

importance(fit_rf_rep_cat01)

```


```{r caret_RFE_rep_cat01}

x=df_train_rep_cat01[, feature_cols_rep_cat01]
y=df_train_rep_cat01[[label]]
results <- rfe(x,y, sizes=c(1:15), rfeControl=control)

print(results)
plot(results)
predictors(results)

```

